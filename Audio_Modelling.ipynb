{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCCMDFnKMjcI"
      },
      "source": [
        "# Audio Modelling VST\n",
        "Create a VST from only MIDI and corresponding audio of an instrument.\n",
        "Dataset: [here](https://magenta.tensorflow.org/datasets/nsynth#instrument-sources).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAw6a07Vr0Fb"
      },
      "source": [
        "# 1. **Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK9hMOgavhZp"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from torchsummary import summary\n",
        "from IPython.display import HTML, Audio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import itertools\n",
        "\n",
        "import glob\n",
        "import json\n",
        "from typing import TypedDict\n",
        "import torchaudio\n",
        "from torchaudio.transforms import Spectrogram, InverseSpectrogram\n",
        "from torchaudio.functional import amplitude_to_DB, DB_to_amplitude\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import requests\n",
        "import tarfile\n",
        "import gc\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw2EDHexJbyM"
      },
      "outputs": [],
      "source": [
        "# !pip install lightning-bolts\n",
        "!pip install git+https://github.com/PytorchLightning/lightning-bolts.git@master --upgrade\n",
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEbRkCM8CkTy"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from pl_bolts.models.gans import Pix2Pix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_5L4I_xsc2i"
      },
      "source": [
        "# 2. **Dataset Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSMYyicpNysm"
      },
      "source": [
        "## **1.** Connect to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qqg2TkEaN0fe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "GDRIVE_DIR = \"/content/drive/\"\n",
        "GDRIVE_HOME_DIR = os.path.join(GDRIVE_DIR, \"MyDrive\")\n",
        "GDRIVE_DATA_DIR = os.path.join(GDRIVE_HOME_DIR, \"AudioModelling\")\n",
        "\n",
        "# Mount drive\n",
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwf_AyD6Mpuo"
      },
      "source": [
        "## **2.** Downloader Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_cWKwW9Mig2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This function downloads a file from a specific URL directly to Google Drive.\n",
        "\"\"\"\n",
        "def get_data(dataset_url, dest, chunk_size=1024):\n",
        "  # Check if file already exists\n",
        "  if os.path.exists(dest):\n",
        "    print(f\"{dest} already exists.\")\n",
        "    return\n",
        "  # Downloading file\n",
        "  with requests.get(dataset_url, stream=True) as r:\n",
        "      r.raise_for_status()\n",
        "      with open(dest, \"wb\") as f:\n",
        "          pbar = tqdm(total=int(r.headers[\"Content-Length\"]), unit=\"B\", unit_scale=True, unit_divisor=1024)\n",
        "          for chunk in r.iter_content(chunk_size=8192):\n",
        "              if chunk:  # filter out keep-alive new chunks\n",
        "                  f.write(chunk)\n",
        "                  pbar.update(len(chunk))\n",
        "\n",
        "\"\"\"\n",
        "This function extract a tar file.\n",
        "\"\"\"\n",
        "def extract_file(tar_path, dest_path, dataset_type):\n",
        "  status_file = os.path.join(dest_path, f\"extract_status_{dataset_type}.txt\")\n",
        "  # Check if already extracted\n",
        "  if os.path.exists(status_file):\n",
        "    print(f\"Data already extracted.\")\n",
        "    return\n",
        "  # Extract the tar file\n",
        "  with tarfile.open(tar_path) as archive:\n",
        "    archive.extractall(dest_path)\n",
        "  # Write file to confirm extraction gone well\n",
        "  with open(status_file, \"w\") as f:\n",
        "    f.write(\"OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU4Kzy1881CR"
      },
      "source": [
        "## **3.** Download Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_1KlMpbqTg8"
      },
      "outputs": [],
      "source": [
        "TRAIN_TYPE=\"valid\" # train, valid, test\n",
        "DATASET_URL = f\"http://download.magenta.tensorflow.org/datasets/nsynth/nsynth-{TRAIN_TYPE}.jsonwav.tar.gz\"\n",
        "GDRIVE_DATA_TRAIN_ZIP = os.path.join(GDRIVE_DATA_DIR, DATASET_URL.split(\"/\")[-1])\n",
        "\n",
        "LOCAL_DATA_DIR = \"/content\"\n",
        "LOCAL_DATA_TRAIN_ZIP = os.path.join(LOCAL_DATA_DIR, DATASET_URL.split(\"/\")[-1])\n",
        "DO_LOCAL_TRAIN = True\n",
        "\n",
        "# Download the dataset in Google Drive\n",
        "get_data(DATASET_URL, GDRIVE_DATA_TRAIN_ZIP)\n",
        "\n",
        "if DO_LOCAL_TRAIN:\n",
        "  # Clone dataset to local colab and extract it\n",
        "  if not os.path.exists(LOCAL_DATA_TRAIN_ZIP):\n",
        "    shutil.copyfile(GDRIVE_DATA_TRAIN_ZIP, LOCAL_DATA_TRAIN_ZIP)\n",
        "  extract_file(LOCAL_DATA_TRAIN_ZIP, LOCAL_DATA_DIR, TRAIN_TYPE)\n",
        "else:\n",
        "  # Extract the dataset in Google Drive\n",
        "  extract_file(GDRIVE_DATA_TRAIN_ZIP, GDRIVE_DATA_DIR, TRAIN_TYPE)\n",
        "  os.makedirs(f\"nsynth-{TRAIN_TYPE}/audio\", exist_ok=True)\n",
        "  shutil.copy(GDRIVE_DATA_DIR + f\"/nsynth-{TRAIN_TYPE}/examples.json\", f\"nsynth-{TRAIN_TYPE}/examples.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Coi8NwR4oyW"
      },
      "source": [
        "## **4.** Download Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX36YI1t4oyf"
      },
      "outputs": [],
      "source": [
        "TEST_TYPE=\"test\" # train, valid, test\n",
        "DATASET_URL = f\"http://download.magenta.tensorflow.org/datasets/nsynth/nsynth-{TEST_TYPE}.jsonwav.tar.gz\"\n",
        "GDRIVE_DATA_TEST_ZIP = os.path.join(GDRIVE_DATA_DIR, DATASET_URL.split(\"/\")[-1])\n",
        "\n",
        "LOCAL_DATA_DIR = \"/content\"\n",
        "LOCAL_DATA_TEST_ZIP = os.path.join(LOCAL_DATA_DIR, DATASET_URL.split(\"/\")[-1])\n",
        "DO_LOCAL_TEST = True\n",
        "\n",
        "# Download the dataset in Google Drive\n",
        "get_data(DATASET_URL, GDRIVE_DATA_TEST_ZIP)\n",
        "\n",
        "if DO_LOCAL_TEST:\n",
        "  # Clone dataset to local colab and extract it\n",
        "  if not os.path.exists(LOCAL_DATA_TEST_ZIP):\n",
        "    shutil.copyfile(GDRIVE_DATA_TEST_ZIP, LOCAL_DATA_TEST_ZIP)\n",
        "  extract_file(LOCAL_DATA_TEST_ZIP, LOCAL_DATA_DIR, TEST_TYPE)\n",
        "else:\n",
        "  # Extract the dataset in Google Drive\n",
        "  extract_file(GDRIVE_DATA_TEST_ZIP, GDRIVE_DATA_DIR, TEST_TYPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESBQLb-BUn1-"
      },
      "source": [
        "## **5.** Dataset Structure\n",
        "The dataset has 11 (different instruments) Ã— 3 (types).\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>\n",
        "      <table>\n",
        "        <thead>\n",
        "          <tr>\n",
        "            <th>Index</th>\n",
        "            <th>ID</th>\n",
        "          </tr>\n",
        "        </thead>\n",
        "        <tbody>\n",
        "          <tr>\n",
        "            <td>0</td>\n",
        "            <td>bass</td>\n",
        "          </tr>\n",
        "          <tr>\n",
        "            <td>1</td>\n",
        "            <td>brass</td>\n",
        "          </tr>\n",
        "          <tr>\n",
        "            <td>2</td>\n",
        "            <td>flute</td>\n",
        "          </tr>\n",
        "          <tr>\n",
        "            <td>3</td>\n",
        "            <td>guitar</td>\n",
        "          </tr>\n",
        "          <tr>\n",
        "            <td>4</td>\n",
        "            <td>keyboard</td>\n",
        "          </tr>\n",
        "          <tr>\n",
        "            <td>5</td>\n",
        "            <td>mallet</td>\n",
        "          </tr>\n",
        "          <tr>\n",
        "            <td>6</td>\n",
        "            <td>organ</td>\n",
        "          </tr>\n",
        "          <tr>\n",
        "            <td>7</td>\n",
        "            <td>reed</td>\n",
        "          </tr>\n",
        "          <tr>\n",
        "            <td>8</td>\n",
        "            <td>string</td>\n",
        "          </tr>\n",
        "          <tr>\n",
        "            <td>9</td>\n",
        "            <td>synth_lead</td>\n",
        "          </tr>\n",
        "          <tr>\n",
        "            <td>10</td>\n",
        "            <td>vocal</td>\n",
        "          </tr>\n",
        "        </tbody>\n",
        "      </table>\n",
        "    </td>\n",
        "    <td width=\"40%\">\n",
        "    </td>\n",
        "    <td>\n",
        "      <table>\n",
        "        <thead>\n",
        "          <tr>\n",
        "            <th>Index</th>\n",
        "            <th>ID</th>\n",
        "          </tr>\n",
        "        </thead>\n",
        "        <tbody>\n",
        "          <tr>\n",
        "            <td>0</td>\n",
        "            <td>acoustic</td>\n",
        "          </tr>\n",
        "          <tr>\n",
        "            <td>1</td>\n",
        "            <td>electronic</td>\n",
        "          </tr>\n",
        "          <tr>\n",
        "            <td>2</td>\n",
        "            <td>synthetic</td>\n",
        "          </tr>\n",
        "        </tbody>\n",
        "      </table>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1m3wTfEZaww"
      },
      "outputs": [],
      "source": [
        "instruments = [ \"bass\", \"brass\", \"flute\", \"guitar\", \"keyboard\", \"mallet\", \"organ\", \"reed\", \"string\", \"synth_lead\", \"vocal\" ]\n",
        "types = [ \"acoustic\", \"electronic\", \"synthetic\" ]\n",
        "\n",
        "# selected_instrument = \"flute_synthetic_000\"\n",
        "selected_instrument = \"guitar_acoustic\"\n",
        "\n",
        "class DataJson(TypedDict):\n",
        "  note: int\n",
        "  sample_rate: int\n",
        "  qualities: list[bool]\n",
        "  pitch: int\n",
        "  note_str: str\n",
        "  instrument_family_str: str\n",
        "\n",
        "train_dataroot = f\"nsynth-{TRAIN_TYPE}/audio/\"\n",
        "test_dataroot = f\"nsynth-{TEST_TYPE}/audio/\"\n",
        "\n",
        "# Load training dataset\n",
        "data = json.load(open(f\"nsynth-{TRAIN_TYPE}/examples.json\"))\n",
        "json_data: list[DataJson] = [x for k, x in data.items() if selected_instrument in x['instrument_str']]\n",
        "sr = [x[\"sample_rate\"] for x in json_data]\n",
        "print(f\"For {selected_instrument} we have {len(json_data)} audio files in training set\")\n",
        "\n",
        "# Load test dataset\n",
        "data = json.load(open(f\"nsynth-{TEST_TYPE}/examples.json\"))\n",
        "json_test_data: list[DataJson] = [x for k, x in data.items() if selected_instrument in x['instrument_str']]\n",
        "print(f\"For {selected_instrument} we have {len(json_test_data)} audio files in test set\")\n",
        "\n",
        "del data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNQWYxnsMqQv"
      },
      "source": [
        "# 3. **PySpark + Colab Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFudfLCRNvXT"
      },
      "source": [
        "## **1.** Install PySpark and related dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6vtwpPGKn9W"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOrmY8FiOMwa"
      },
      "source": [
        "## **2.** Import useful PySpark packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh8zPg5APmYv"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbT2rM-4tvnB"
      },
      "source": [
        "## **3.** Create Spark context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXQOJijlEz3I"
      },
      "outputs": [],
      "source": [
        "# Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '4G').\\\n",
        "                set('spark.driver.memory', '45G').\\\n",
        "                set('spark.driver.maxResultSize', '10G').\\\n",
        "                setAppName(\"AudioModelling\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXIVVHF-ZrVt"
      },
      "source": [
        "## **4.** Create Web UI Console"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_nquJJkZ906"
      },
      "outputs": [],
      "source": [
        "# Install ngrok\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHV-6DGRcPY4"
      },
      "outputs": [],
      "source": [
        "# Auth on ngrok\n",
        "!ngrok authtoken 2MyVytznaXPBeUvYCOpAzAXYMrH_6iBW9P8793yjNfBAoemCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw66RiCKeZ_X"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a ngrok tunnel on the port 4050 where Spark is running\n",
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imGZyiJSguni"
      },
      "outputs": [],
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\n\\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eRvyl2xwCV6"
      },
      "source": [
        "## **5.** Check everything is ok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fqJ5f0JE3BL"
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhTN342EEOYZ"
      },
      "outputs": [],
      "source": [
        "sc._conf.getAll()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geKkovOI5uy7"
      },
      "source": [
        "# 4. **Audio Utilities**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load Audio"
      ],
      "metadata": {
        "id": "Zecrx-34RysT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbuBVVV5fhD_"
      },
      "outputs": [],
      "source": [
        "def load_audio(audio_path, normalize=True):\n",
        "  wav, _ = torchaudio.load(audio_path, normalize=normalize)\n",
        "  return wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulMw3cG4R5dC"
      },
      "source": [
        "## 2. Show Spectrogram and Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Osp6KHqHnFFS"
      },
      "outputs": [],
      "source": [
        "def show_spec_and_audio(spec, phase, title=None, ylabel=\"freq_bin\", aspect=\"auto\", xmax=None):\n",
        "  # Sine audio by midi spec\n",
        "  fig, axs = plt.subplots(1, 1)\n",
        "  axs.set_title(title or \"Spectrogram (db)\")\n",
        "  axs.set_ylabel(ylabel)\n",
        "  axs.set_xlabel(\"frame\")\n",
        "  im = axs.imshow(spec, origin=\"lower\", aspect=aspect)\n",
        "  if xmax:\n",
        "      axs.set_xlim((0, xmax))\n",
        "  fig.colorbar(im, ax=axs)\n",
        "  plt.show(block=False)\n",
        "\n",
        "  # Reconstructed original audio\n",
        "  wav_orig_reconstruct = audioT.spec_to_wav(spec, phase)\n",
        "  display(Audio(wav_orig_reconstruct, rate=midi_item[\"sample_rate\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRwsAXylrowq"
      },
      "source": [
        "## 3. Generate Audio from Midi  (sawtooth synth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjjNIRSHru5t"
      },
      "outputs": [],
      "source": [
        "from scipy import signal\n",
        "\n",
        "def midi_to_audio(pitch, velocity, duration=2, sample_rate=16000):\n",
        "  freq = 440*(2**((pitch-69)/12))\n",
        "  velocity /= 127\n",
        "  t = np.linspace(0., duration, int(sample_rate * duration))\n",
        "  # sample = signal.sawtooth(freq * 2. * np.pi * t ) * velocity\n",
        "  sample = np.sin(freq * 2. * np.pi * t ) * velocity\n",
        "  return torch.tensor([sample])\n",
        "\n",
        "Audio(midi_to_audio(70, 127), rate=16000, normalize=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2_vCBXY50cS"
      },
      "source": [
        "## 3. Audio-Melspectrogram conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmK4AD__5_b1"
      },
      "outputs": [],
      "source": [
        "class AudioTransform():\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.n_fft = 1024\n",
        "    self.hop_length = self.n_fft//4\n",
        "    self.spectrogram = Spectrogram(n_fft=self.n_fft, hop_length=self.hop_length)\n",
        "    self.inverse_spectrogram = InverseSpectrogram(n_fft=self.n_fft, hop_length=self.hop_length)\n",
        "\n",
        "  def wav_to_spec(self, wav, sample_rate=16000):\n",
        "    # Transform audio to spec\n",
        "    S = self.spectrogram(wav).squeeze()\n",
        "    S, phase = torch.abs(S), torch.angle(S)\n",
        "    # Create a (h=2x, w=x) size array\n",
        "    blank_column = torch.zeros((S.shape[0], 5))\n",
        "    S = torch.cat((S, blank_column), axis=1)[:self.n_fft//2]\n",
        "    phase = torch.cat((phase, blank_column), axis=1)[:self.n_fft//2]\n",
        "    # Get amplitude\n",
        "    S = amplitude_to_DB(S, multiplier=10, amin=0, db_multiplier=torch.log10(torch.max(S))) + 80\n",
        "    return S, phase\n",
        "\n",
        "  def spec_to_wav(self, spec, phase, sample_rate=16000):\n",
        "    # Transform spec to audio\n",
        "    A = DB_to_amplitude(spec - 80, ref=torch.max(spec), power=1)\n",
        "    A = spec * np.exp(1j*phase)\n",
        "    blank_column = torch.zeros((1, A.shape[1]))\n",
        "    A = torch.cat((A, blank_column), axis=0)\n",
        "    A = self.inverse_spectrogram(A)\n",
        "    return A\n",
        "\n",
        "# Load audio from file\n",
        "midi_item = json_data[-6]\n",
        "print(midi_item[\"note_str\"])\n",
        "filename = midi_item[\"note_str\"] + \".wav\"\n",
        "wav_orig = load_audio(train_dataroot + filename)\n",
        "\n",
        "# Load midi info from file\n",
        "duration = wav_orig.shape[1]/midi_item[\"sample_rate\"]\n",
        "wav_midi = midi_to_audio(midi_item[\"pitch\"], midi_item[\"velocity\"], duration, midi_item[\"sample_rate\"])\n",
        "\n",
        "print(\"Original audio and midi audio synthesized\")\n",
        "# Original audio\n",
        "display(Audio(wav_orig, rate=midi_item[\"sample_rate\"]))\n",
        "# Sine audio by midi\n",
        "display(Audio(wav_midi, rate=midi_item[\"sample_rate\"]))\n",
        "\n",
        "audioT = AudioTransform()\n",
        "\n",
        "print(\"Spectrogram and inverse spectrogram to get original audio and midi audio synthesized back\")\n",
        "# Original audio spec\n",
        "spec_orig, phase = audioT.wav_to_spec(wav_orig, midi_item[\"sample_rate\"])\n",
        "show_spec_and_audio(spec_orig, phase)\n",
        "# Sine audio by midi spec\n",
        "spec_midi, phase = audioT.wav_to_spec(wav_midi, midi_item[\"sample_rate\"])\n",
        "show_spec_and_audio(spec_midi, phase)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wav_midi = midi_to_audio(127, midi_item[\"velocity\"], duration, midi_item[\"sample_rate\"])\n",
        "# spec_midi, phase = audioT.wav_to_spec(wav_midi, midi_item[\"sample_rate\"])\n",
        "# split = split_low_high_freq(torch.tensor(spec_midi)).numpy()\n",
        "# split_phase = split_low_high_freq(torch.tensor(phase)).numpy()\n",
        "# show_spec_and_audio(split[0], split_phase[0])\n",
        "# show_spec_and_audio(split[1], split_phase[1])"
      ],
      "metadata": {
        "id": "J67DU1hf00hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgtHW15M0AeX"
      },
      "source": [
        "# 5. **Model Utilities**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuunEhiteF48"
      },
      "source": [
        "## 1. Patch Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQsfGbPiDX03"
      },
      "outputs": [],
      "source": [
        "def split_low_high_freq(spectrum):\n",
        "    \"\"\"\n",
        "    Split in half the 2d spectrum and add as new dimension.\n",
        "\n",
        "    Args:\n",
        "      spectrum: shape (1, h, w) where h=2*w\n",
        "\n",
        "    Return:\n",
        "      spectrum_3d: shape (2, w, w)\n",
        "    \"\"\"\n",
        "    x, h, w = spectrum.shape\n",
        "    # Check if h is two times w\n",
        "    if (h!=2*w):\n",
        "      raise Exception(f\"Invalid array of shape {spectrum.shape}\")\n",
        "    spectrum_3d = torch.reshape(spectrum, (2,w,w))\n",
        "    return spectrum_3d\n",
        "\n",
        "def merge_low_high_freq(spectrum_3d):\n",
        "    \"\"\"\n",
        "    Merge the splitted spectrum and remove third dimension.\n",
        "\n",
        "    Args:\n",
        "      spectrum_3d: shape (2, w, w)\n",
        "\n",
        "    Return:\n",
        "      spectrum: shape (h, w) where h=2*w\n",
        "    \"\"\"\n",
        "    x, h, w = spectrum_3d.shape\n",
        "    spectrum = torch.reshape(spectrum_3d, (1, w*2,w))\n",
        "    return spectrum\n",
        "\n",
        "def split_remove_high_freq(spectrum):\n",
        "    \"\"\"\n",
        "    Split in half the 2d spectrum, add as new dimension and remove the high frequency part.\n",
        "\n",
        "    Args:\n",
        "      spectrum: shape (1, h, w) where h=2*w\n",
        "\n",
        "    Return:\n",
        "      spectrum_3d: shape (1, w, w)\n",
        "    \"\"\"\n",
        "    spectrum_3d = split_low_high_freq(spectrum)[0][None]\n",
        "    return spectrum_3d\n",
        "\n",
        "def recreate_high_freq(spectrum_3d):\n",
        "    \"\"\"\n",
        "    Recreate the high frequency part of the spectrum and remove third dimension.\n",
        "\n",
        "    Args:\n",
        "      spectrum_3d: shape (1, w, w)\n",
        "\n",
        "    Return:\n",
        "      spectrum: shape (h, w) where h=2*w and the top of the matrix is full of zeros\n",
        "    \"\"\"\n",
        "    x, h, w = spectrum_3d.shape\n",
        "    zeros_tensor = torch.zeros(1, h, w)\n",
        "    spectrum = torch.cat((spectrum_3d, zeros_tensor), dim=0)\n",
        "    return spectrum\n",
        "\n",
        "n = 4\n",
        "values = [i for i in range(-16,16)]\n",
        "arr = [values[n*i:n*(i+1)] for i in range(len(values)//n)]\n",
        "x = torch.tensor([arr])\n",
        "print(\"Original:\\n\", x, x.shape)\n",
        "x = split_low_high_freq(x)\n",
        "print(\"After Splitting:\\n\", x, x.shape)\n",
        "x = merge_low_high_freq(x)\n",
        "print(\"After Merging:\\n\", x, x.shape)\n",
        "x = split_remove_high_freq(x)\n",
        "print(\"After Removing High Frequencies:\\n\", x, x.shape)\n",
        "x = recreate_high_freq(x)\n",
        "print(\"After Recreating (Zeros) High Frequencies:\\n\", x, x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataloaders"
      ],
      "metadata": {
        "id": "Y4LfDre_SA2Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uORnds6BcutE"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataroot, json):\n",
        "        self.audio_path = dataroot\n",
        "        self.data = [] # list of [spec_midi, spec_orig]\n",
        "        self.phases = [] # list of [phase_midi, phase_orig]\n",
        "        for data in json:\n",
        "            # Get data\n",
        "            filename, pitch, velocity, sr = f\"{data['note_str']}.wav\", data[\"pitch\"], data[\"velocity\"], data[\"sample_rate\"]\n",
        "            # Load audio\n",
        "            if (not os.path.exists(self.audio_path + filename)):\n",
        "              continue\n",
        "            wav_orig = load_audio(self.audio_path + filename)\n",
        "            # Get duration\n",
        "            duration = wav_orig.shape[1]/sr\n",
        "            # Synth midi audio\n",
        "            wav_midi = midi_to_audio(pitch, velocity, duration, sr)\n",
        "            # Convert to spectrum\n",
        "            spec_orig, phase_orig = audioT.wav_to_spec(wav_orig)\n",
        "            spec_midi, phase_midi = audioT.wav_to_spec(wav_midi)\n",
        "            # Zero padding to make it a square\n",
        "            spec_orig = split_remove_high_freq(spec_orig[None]) # Take only the half with meaningful signal\n",
        "            spec_midi = split_remove_high_freq(spec_midi[None]) # Take only the half with meaningful signal\n",
        "            # spec_midi = split_low_high_freq(spec_midi)\n",
        "            self.data.append((spec_orig, spec_midi))\n",
        "            # Phases\n",
        "            self.phases.append((phase_orig, phase_midi))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        spec_orig, spec_midi = self.data[idx]\n",
        "        return spec_orig, spec_midi\n",
        "\n",
        "    def get_phase(self, idx):\n",
        "        phase_orig, phase_midi = self.phases[idx]\n",
        "        return phase_orig, phase_midi\n",
        "\n",
        "train_dataset = CustomDataset(train_dataroot, json_data)\n",
        "print(len(train_dataset))\n",
        "test_dataset = CustomDataset(test_dataroot, json_test_data)\n",
        "print(len(test_dataset))\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 128\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "train_sample = next(iter(train_dataloader))\n",
        "# test_sample = test_dataset[0]\n",
        "# test_phase = test_dataset.get_phase(0)\n",
        "\n",
        "min_in = float('inf')\n",
        "max_in = -float('inf')\n",
        "min_out = float('inf')\n",
        "max_out = -float('inf')\n",
        "for input, output in train_dataloader:\n",
        "    # For input\n",
        "    max_data = input.max()\n",
        "    min_data = input.min()\n",
        "    min_in = min_data if min_data < min_in else min_in\n",
        "    max_in = max_data if max_data > max_in else max_in\n",
        "    # For output\n",
        "    max_data = output.max()\n",
        "    min_data = output.min()\n",
        "    min_out = min_data if min_data < min_out else min_out\n",
        "    max_out = max_data if max_data > max_out else max_out\n",
        "print(f\"Input shape ({input.shape}) | min {min_in} | max {max_in}\")\n",
        "print(f\"Output shape ({output.shape}) | min {min_out} | max {max_out}\")\n",
        "\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Custom Activation Function"
      ],
      "metadata": {
        "id": "WsyKHBX6Sy8r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BW0KkrQzAot"
      },
      "outputs": [],
      "source": [
        "class CustomAct(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate the current minimum and maximum values of the matrix\n",
        "        x_min = torch.min(x)\n",
        "        x_max = torch.max(x)\n",
        "\n",
        "        # Normalize the matrix to the range [0, 1]\n",
        "        x = (x - x_min) / (x_max - x_min)\n",
        "\n",
        "        x = x * 80\n",
        "\n",
        "        return x\n",
        "\n",
        "customAct = CustomAct()\n",
        "for i in range(1):\n",
        "  if torch.cuda.is_available():\n",
        "    m = torch.rand((8,2,256,256)).to(\"cuda\")\n",
        "  else:\n",
        "    m = torch.rand((1,2,256,256))\n",
        "  m = customAct.forward(m)\n",
        "  print(m.max(), m.min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaRh26FttS57"
      },
      "source": [
        "# 6. Pix2Pix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. LR Finder adjustment"
      ],
      "metadata": {
        "id": "W6Iy5KL9TKeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pytorch_lightning.tuner.lr_finder import _LRFinder, _LinearLR, _ExponentialLR, LRScheduler\n",
        "# from pytorch_lightning.utilities.types import LRScheduler, LRSchedulerConfig, STEP_OUTPUT\n",
        "# from typing import cast\n",
        "\n",
        "\n",
        "# def new_exchange_scheduler(self, trainer: \"pl.Trainer\") -> None:\n",
        "#         # TODO: update docs here\n",
        "#         \"\"\"Decorate `trainer.strategy.setup_optimizers` method such that it sets the user's originally specified\n",
        "#         optimizer together with a new scheduler that takes care of the learning rate search.\"\"\"\n",
        "\n",
        "#         optimizers = trainer.strategy.optimizers\n",
        "\n",
        "#         optimizer = optimizers[0]\n",
        "\n",
        "#         new_lrs = [self.lr_min] * len(optimizer.param_groups)\n",
        "#         for param_group, new_lr in zip(optimizer.param_groups, new_lrs):\n",
        "#             param_group[\"lr\"] = new_lr\n",
        "#             param_group[\"initial_lr\"] = new_lr\n",
        "\n",
        "#         args = (optimizer, self.lr_max, self.num_training)\n",
        "#         scheduler = _LinearLR(*args) if self.mode == \"linear\" else _ExponentialLR(*args)\n",
        "#         scheduler = cast(LRScheduler, scheduler)\n",
        "\n",
        "#         trainer.strategy.optimizers = [optimizer]\n",
        "#         trainer.strategy.lr_scheduler_configs = [LRSchedulerConfig(scheduler, interval=\"step\")]\n",
        "\n",
        "# _LRFinder._exchange_scheduler = new_exchange_scheduler\n",
        "\n",
        "# def new_training_step(self, batch, batch_idx, optimizer_idx=0):\n",
        "#     real, condition = batch\n",
        "\n",
        "#     loss = None\n",
        "#     if optimizer_idx == 0:\n",
        "#         loss = self._disc_step(real, condition)\n",
        "#         self.log(\"PatchGAN Loss\", loss)\n",
        "#     elif optimizer_idx == 1:\n",
        "#         loss = self._gen_step(real, condition)\n",
        "#         self.log(\"Generator Loss\", loss)\n",
        "\n",
        "#     return loss\n",
        "\n",
        "# Pix2Pix.training_step = new_training_step"
      ],
      "metadata": {
        "id": "0iHLC1sa8IQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pytorch_lightning.tuner.lr_finder import _LRFinder, _LinearLR, _ExponentialLR, LRScheduler\n",
        "# from pytorch_lightning.utilities.types import LRScheduler, LRSchedulerConfig, STEP_OUTPUT\n",
        "# from typing import cast\n",
        "\n",
        "\n",
        "# def new_exchange_scheduler(self, trainer: \"pl.Trainer\") -> None:\n",
        "#         \"\"\"Decorate `trainer.strategy.setup_optimizers` method such that it sets the user's originally specified\n",
        "#         optimizer together with a new scheduler that takes care of the learning rate search.\"\"\"\n",
        "\n",
        "#         optimizers = trainer.strategy.optimizers\n",
        "#         trainer.strategy.lr_scheduler_configs = []\n",
        "\n",
        "#         for optimizer in optimizers:\n",
        "#             new_lrs = [self.lr_min] * len(optimizer.param_groups)\n",
        "#             for param_group, new_lr in zip(optimizer.param_groups, new_lrs):\n",
        "#                 param_group[\"lr\"] = new_lr\n",
        "#                 param_group[\"initial_lr\"] = new_lr\n",
        "\n",
        "#             args = (optimizer, self.lr_max, self.num_training)\n",
        "#             scheduler = _LinearLR(*args) if self.mode == \"linear\" else _ExponentialLR(*args)\n",
        "#             scheduler = cast(LRScheduler, scheduler)\n",
        "#             trainer.strategy.lr_scheduler_configs.append(LRSchedulerConfig(scheduler, interval=\"step\"))\n",
        "\n",
        "# _LRFinder._exchange_scheduler = new_exchange_scheduler"
      ],
      "metadata": {
        "id": "dGSVN3liuhDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Run learning rate finder\n",
        "# lr_finder = trainer.tuner.lr_find(pix2pix, train_dataloader, min_lr=1e-100, max_lr=1e-5, early_stop_threshold=None)\n",
        "\n",
        "# # Plot with\n",
        "# fig = lr_finder.plot(suggest=True)\n",
        "# fig.show()\n",
        "\n",
        "# # Pick point based on plot, or get suggestion\n",
        "# new_lr = lr_finder.suggestion()\n",
        "\n",
        "# # update hparams of the model\n",
        "# pix2pix.hparams.lr = new_lr\n",
        "# pix2pix.hparams.learning_rate = new_lr\n",
        "# pix2pix.lr = new_lr\n",
        "# pix2pix.learning_rate = new_lr\n",
        "# print(new_lr)"
      ],
      "metadata": {
        "id": "EkXx4xX4l5Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model creation"
      ],
      "metadata": {
        "id": "m7UXInqkTSiR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBXfVv2taeLc"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  pix2pix = Pix2Pix(in_channels=1, out_channels=1, learning_rate=1e-3).to(\"cuda\")\n",
        "else:\n",
        "  pix2pix = Pix2Pix(in_channels=1, out_channels=1, learning_rate=1e-3)\n",
        "\n",
        "# pix2pix.gen.tanh = CustomAct() #TODO TEST WITH THIS\n",
        "pix2pix.gen.tanh = nn.ReLU()\n",
        "\n",
        "# print(pix2pix.gen)\n",
        "\n",
        "# summary(pix2pix.gen, (1,256,256))\n",
        "# summary(pix2pix.patch_gan, [(2,256,256), (2,256,256)])\n",
        "\n",
        "!rm -R checkpoints/pix2pix\n",
        "!rm -R checkpoints/pix2pix/lightning_logs/version_1\n",
        "# !cp -R \"checkpoints/pix2pix/lightning_logs/version_1/checkpoints/epoch=249-step=2000.ckpt\" \"drive/MyDrive/AudioModelling/dst.ckpt\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  trainer = pl.Trainer(max_epochs=200, default_root_dir=\"checkpoints/pix2pix\", accelerator=\"gpu\", auto_lr_find=True)\n",
        "else:\n",
        "  trainer = pl.Trainer(max_epochs=200, default_root_dir=\"checkpoints/pix2pix\", auto_lr_find=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training"
      ],
      "metadata": {
        "id": "GiXRNSGOTWCW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agQVlwwzPu_V"
      },
      "outputs": [],
      "source": [
        "# Load checkpoints\n",
        "# checkpoint_folder = \"checkpoints/pix2pix/lightning_logs\"\n",
        "# if os.path.exists(checkpoint_folder):\n",
        "#   last_version = f\"/version_{len(os.listdir(checkpoint_folder))-1}/checkpoints/\"\n",
        "#   last_checkpoint = os.listdir(checkpoint_folder+last_version)[-1]\n",
        "#   print(f\"Loaded checkpoint {last_version + last_checkpoint}\")\n",
        "#   trainer.fit(pix2pix, train_dataloader, ckpt_path=checkpoint_folder+last_version+last_checkpoint)\n",
        "# else:\n",
        "#   trainer.fit(pix2pix, train_dataloader)\n",
        "\n",
        "trainer.fit(pix2pix, train_dataloader)\n",
        "\n",
        "# !cp -R checkpoints/pix2pix dst"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Testing"
      ],
      "metadata": {
        "id": "fB5JK3HyTYyv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4Ift66DA7a0"
      },
      "outputs": [],
      "source": [
        "idx = np.random.randint(0,len(train_dataset))\n",
        "print(idx)\n",
        "spec_orig, spec_midi = train_dataset[idx] #TO REMOVE\n",
        "phase_orig, phase_midi = train_dataset.get_phase(idx) #TO REMOVE\n",
        "norm = CustomAct()\n",
        "if torch.cuda.is_available():\n",
        "  # Generate from data\n",
        "  data = spec_midi.unsqueeze(dim=0).to(\"cuda\") # Create batch with size one\n",
        "  generated_spec = pix2pix.to(\"cuda\").gen(data).cpu().detach()[0] # Take first element of the batch\n",
        "  # Some stats to remove\n",
        "  print(0, generated_spec[0].max(), generated_spec[0].min())\n",
        "  max_data = generated_spec.max()\n",
        "  min_data = generated_spec.min()\n",
        "  print(max_data, min_data)\n",
        "  # Normalize it\n",
        "  generated_spec = norm(generated_spec)#-spec_midi*generated_spec*0.01)\n",
        "else:\n",
        "  # Generate from data\n",
        "  data = spec_midi.unsqueeze(dim=0) # Create batch with size one\n",
        "  generated_spec = pix2pix.gen(data).cpu().detach()[0] # Take first element of the batch\n",
        "  generated_spec = norm(merge_low_high_freq(generated_spec)).numpy() # Normalize it\n",
        "\n",
        "generated_spec = merge_low_high_freq(recreate_high_freq(generated_spec)).numpy()\n",
        "show_spec_and_audio(generated_spec, phase_orig)\n",
        "# # Original source sample\n",
        "spec_midi = merge_low_high_freq(recreate_high_freq(spec_midi)).numpy()\n",
        "show_spec_and_audio(spec_midi, phase_midi)\n",
        "# Original dest sample\n",
        "spec_orig =  merge_low_high_freq(recreate_high_freq(spec_orig)).numpy()\n",
        "show_spec_and_audio(spec_orig, phase_orig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sKVKt4Bb_4R"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for i in range(5):\n",
        "  idx = np.random.randint(0,len(train_dataset))\n",
        "  print(idx)\n",
        "  spec_orig, spec_midi = train_dataset[idx] #TO REMOVE\n",
        "  phase_orig, phase_midi = train_dataset.get_phase(idx) #TO REMOVE\n",
        "  sample = merge_low_high_freq(recreate_high_freq(spec_orig.cpu().detach())).numpy()\n",
        "  # dest_sample = merge_low_high_freq(recreate_high_freq(spec_midi)).numpy()\n",
        "  show_spec_and_audio(sample, phase_orig)"
      ],
      "metadata": {
        "id": "3Ka84gGc6FbE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SAw6a07Vr0Fb",
        "O_5L4I_xsc2i",
        "KSMYyicpNysm",
        "vwf_AyD6Mpuo",
        "AU4Kzy1881CR",
        "8Coi8NwR4oyW",
        "ESBQLb-BUn1-",
        "iNQWYxnsMqQv",
        "tFudfLCRNvXT",
        "YOrmY8FiOMwa",
        "hbT2rM-4tvnB",
        "LXIVVHF-ZrVt",
        "_eRvyl2xwCV6",
        "Zecrx-34RysT",
        "lRwsAXylrowq",
        "CuunEhiteF48",
        "Y4LfDre_SA2Q",
        "WsyKHBX6Sy8r",
        "W6Iy5KL9TKeT"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}